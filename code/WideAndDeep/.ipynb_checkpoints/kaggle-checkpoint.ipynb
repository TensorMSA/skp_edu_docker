{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import gc\n",
    "\n",
    "print('Loading data ...')\n",
    "\n",
    "train = pd.read_csv('../input/train_2016.csv')\n",
    "prop = pd.read_csv('../input/properties_2016.csv')\n",
    "sample = pd.read_csv('../input/sample_submission.csv')\n",
    "\n",
    "print('Binding to float32')\n",
    "\n",
    "for c, dtype in zip(prop.columns, prop.dtypes):\n",
    "\tif dtype == np.float64:\n",
    "\t\tprop[c] = prop[c].astype(np.float32)\n",
    "\n",
    "print('Creating training set ...')\n",
    "\n",
    "df_train = train.merge(prop, how='left', on='parcelid')\n",
    "\n",
    "x_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode'], axis=1)\n",
    "y_train = df_train['logerror'].values\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "train_columns = x_train.columns\n",
    "\n",
    "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n",
    "    x_train[c] = (x_train[c] == True)\n",
    "\n",
    "del df_train; gc.collect()\n",
    "\n",
    "split = 80000\n",
    "x_train, y_train, x_valid, y_valid = x_train[:split], y_train[:split], x_train[split:], y_train[split:]\n",
    "\n",
    "print('Building DMatrix...')\n",
    "\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n",
    "\n",
    "del x_train, x_valid; gc.collect()\n",
    "\n",
    "print('Training ...')\n",
    "\n",
    "params = {}\n",
    "params['eta'] = 0.02\n",
    "params['objective'] = 'reg:linear'\n",
    "params['eval_metric'] = 'mae'\n",
    "params['max_depth'] = 4\n",
    "params['silent'] = 1\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "clf = xgb.train(params, d_train, 10000, watchlist, early_stopping_rounds=100, verbose_eval=10)\n",
    "\n",
    "del d_train, d_valid\n",
    "\n",
    "print('Building test set ...')\n",
    "\n",
    "sample['parcelid'] = sample['ParcelId']\n",
    "df_test = sample.merge(prop, on='parcelid', how='left')\n",
    "\n",
    "del prop; gc.collect()\n",
    "\n",
    "x_test = df_test[train_columns]\n",
    "for c in x_test.dtypes[x_test.dtypes == object].index.values:\n",
    "    x_test[c] = (x_test[c] == True)\n",
    "\n",
    "del df_test, sample; gc.collect()\n",
    "\n",
    "d_test = xgb.DMatrix(x_test)\n",
    "\n",
    "del x_test; gc.collect()\n",
    "\n",
    "print('Predicting on test ...')\n",
    "\n",
    "p_test = clf.predict(d_test)\n",
    "\n",
    "del d_test; gc.collect()\n",
    "\n",
    "sub = pd.read_csv('../input/sample_submission.csv')\n",
    "for c in sub.columns[sub.columns != 'ParcelId']:\n",
    "    sub[c] = p_test\n",
    "\n",
    "print('Writing csv ...')\n",
    "sub.to_csv('xgb_starter.csv', index=False, float_format='%.4f') # Thanks to @inversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kaggle 도전\n",
    "\n",
    "## Census Income Data를 가지고 소득 5만불이상과 이하를 분류하는 모델\n",
    "* Listing of attributes: \n",
    "\n",
    "| attribute | values   |\n",
    "|---------------|------|\n",
    "|label          |  >50K, <=50K.|\n",
    "|age            | continuous|\n",
    "|workclass      | Private, Self-emp-not-inc, Self-emp-inc ... |\n",
    "|education      | Bachelors, Some-college, 11th ...|\n",
    "|Doctorate      | 5th-6th, Preschool|\n",
    "|education-num  | continuous|\n",
    "|marital-status | Married-civ-spouse, Divorced|\n",
    "|occupation     | Tech-support,|\n",
    "|clerical       |  Farming-fishing|\n",
    "|relationship   | Wife, Own-child,|\n",
    "|race           | White, Asian-Pac-Islander|\n",
    "|sex            | Female, Male|\n",
    "|capital-gain   | continuous|\n",
    "|capital-loss   | continuous|\n",
    "|hours-per-week | continuous|\n",
    "|native-country | United-States, Cambodia, England|\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (22,32,34,49,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training set ...\n",
      "(90275, 55) (90275,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "print('Loading data ...')\n",
    "\n",
    "train = pd.read_csv('../input/train_2016_v2.csv')\n",
    "prop = pd.read_csv('../input/properties_2016.csv')\n",
    "sample = pd.read_csv('../input/sample_submission.csv')\n",
    "\n",
    "#print(\"rowcount {0}\".format(len(train.index)))\n",
    "\n",
    "# print('Binding to float32')\n",
    "\n",
    "# for c, dtype in zip(prop.columns, prop.dtypes):\n",
    "#     if dtype == np.float64:\n",
    "#         prop[c] = prop[c].astype(np.float32)\n",
    "\n",
    "print('Creating training set ...')\n",
    "\n",
    "df_train = train.merge(prop, how='left', on='parcelid')\n",
    "\n",
    "x_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode'], axis=1)\n",
    "y_train = df_train['logerror'].values\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_columns = x_train.columns\n",
    "\n",
    "#print(x_train.dtypes)\n",
    "#print(dir(x_train.dtypes))\n",
    "\n",
    "CATEGORICAL_COLUMNS = [ _name for _name, _type in x_train.dtypes.iteritems() if _type == 'object' ]\n",
    "#print(CATEGORICAL_COLUMNS)\n",
    "# 여기서 column을 바꾸자\n",
    "\n",
    "CONTINUOUS_COLUMNS = [ _name for _name, _type in x_train.dtypes.iteritems() if _type != 'object' ]\n",
    "#print(CONTINUOUS_COLUMNS)\n",
    "\n",
    "LABEL_COLUMN = 'logerror'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_estimator(model_dir, model_type):\n",
    "  \"\"\"Build an estimator.\"\"\"\n",
    "  # Sparse base columns.\n",
    "    \n",
    "    #['airconditioningtypeid', 'architecturalstyletypeid', 'basementsqft', 'bathroomcnt', 'bedroomcnt', 'buildingclasstypeid'\n",
    "  airconditioningtypeid = tf.contrib.layers.real_valued_column(\"airconditioningtypeid\")\n",
    "  architecturalstyletypeid = tf.contrib.layers.real_valued_column(\"architecturalstyletypeid\")\n",
    "  basementsqft = tf.contrib.layers.real_valued_column(\"basementsqft\")\n",
    "  bathroomcnt = tf.contrib.layers.real_valued_column(\"bathroomcnt\")\n",
    "  bedroomcnt = tf.contrib.layers.real_valued_column(\"bedroomcnt\")\n",
    "  buildingclasstypeid = tf.contrib.layers.real_valued_column(\"buildingclasstypeid\")\n",
    "    \n",
    "   #, 'buildingqualitytypeid', 'calculatedbathnbr', 'decktypeid', 'finishedfloor1squarefeet', 'calculatedfinishedsquarefeet', 'finishedsquarefeet12', 'finishedsquarefeet13',\n",
    "  buildingqualitytypeid = tf.contrib.layers.real_valued_column(\"buildingqualitytypeid\")\n",
    "  calculatedbathnbr = tf.contrib.layers.real_valued_column(\"calculatedbathnbr\")\n",
    "  decktypeid = tf.contrib.layers.real_valued_column(\"decktypeid\")\n",
    "  finishedfloor1squarefeet = tf.contrib.layers.real_valued_column(\"finishedfloor1squarefeet\")\n",
    "  calculatedfinishedsquarefeet = tf.contrib.layers.real_valued_column(\"calculatedfinishedsquarefeet\")\n",
    "  finishedsquarefeet12 = tf.contrib.layers.real_valued_column(\"finishedsquarefeet12\")\n",
    "  finishedsquarefeet13 = tf.contrib.layers.real_valued_column(\"finishedsquarefeet13\")\n",
    "\n",
    "# 'finishedsquarefeet15', 'finishedsquarefeet50', 'finishedsquarefeet6', 'fips', 'fireplacecnt', 'fullbathcnt', 'garagecarcnt',\n",
    "    \n",
    "  finishedsquarefeet15 = tf.contrib.layers.real_valued_column(\"finishedsquarefeet15\")\n",
    "  finishedsquarefeet50 = tf.contrib.layers.real_valued_column(\"finishedsquarefeet50\")\n",
    "  finishedsquarefeet6 = tf.contrib.layers.real_valued_column(\"finishedsquarefeet6\")\n",
    "  fips = tf.contrib.layers.real_valued_column(\"fips\")\n",
    "  fireplacecnt = tf.contrib.layers.real_valued_column(\"fireplacecnt\")\n",
    "  fullbathcnt = tf.contrib.layers.real_valued_column(\"fullbathcnt\")\n",
    "  garagecarcnt = tf.contrib.layers.real_valued_column(\"garagecarcnt\")\n",
    "\n",
    "# 'garagetotalsqft', 'heatingorsystemtypeid', 'latitude', 'longitude', 'lotsizesquarefeet', 'poolcnt', 'poolsizesum', 'pooltypeid10'\n",
    "\n",
    "  garagetotalsqft = tf.contrib.layers.real_valued_column(\"garagetotalsqft\")\n",
    "  heatingorsystemtypeid = tf.contrib.layers.real_valued_column(\"heatingorsystemtypeid\")\n",
    "  latitude = tf.contrib.layers.real_valued_column(\"latitude\")\n",
    "  longitude = tf.contrib.layers.real_valued_column(\"longitude\")\n",
    "  lotsizesquarefeet = tf.contrib.layers.real_valued_column(\"lotsizesquarefeet\")\n",
    "  poolcnt = tf.contrib.layers.real_valued_column(\"poolcnt\")\n",
    "  garagecarcnt = tf.contrib.layers.real_valued_column(\"garagecarcnt\")\n",
    "  poolsizesum = tf.contrib.layers.real_valued_column(\"poolsizesum\")\n",
    "  pooltypeid10 = tf.contrib.layers.real_valued_column(\"pooltypeid10\")\n",
    "    \n",
    "    \n",
    "    \n",
    "#, 'pooltypeid2', 'pooltypeid7', 'propertylandusetypeid', 'rawcensustractandblock', 'regionidcity', 'regionidcounty', \n",
    "  pooltypeid2 = tf.contrib.layers.real_valued_column(\"pooltypeid2\")\n",
    "  pooltypeid7 = tf.contrib.layers.real_valued_column(\"pooltypeid7\")\n",
    "  propertylandusetypeid = tf.contrib.layers.real_valued_column(\"propertylandusetypeid\")\n",
    "  rawcensustractandblock = tf.contrib.layers.real_valued_column(\"rawcensustractandblock\")\n",
    "  regionidcity = tf.contrib.layers.real_valued_column(\"regionidcity\")\n",
    "  regionidcounty = tf.contrib.layers.real_valued_column(\"regionidcounty\")\n",
    "  regionidneighborhood = tf.contrib.layers.real_valued_column(\"regionidneighborhood\")\n",
    "    \n",
    "#'regionidneighborhood', 'regionidzip', 'roomcnt', 'storytypeid', 'threequarterbathnbr', 'typeconstructiontypeid', 'unitcnt', 'yardbuildingsqft17'\n",
    "  regionidneighborhood = tf.contrib.layers.real_valued_column(\"regionidneighborhood\")\n",
    "  regionidzip = tf.contrib.layers.real_valued_column(\"regionidzip\")\n",
    "  roomcnt = tf.contrib.layers.real_valued_column(\"roomcnt\")\n",
    "  storytypeid = tf.contrib.layers.real_valued_column(\"storytypeid\")\n",
    "  threequarterbathnbr = tf.contrib.layers.real_valued_column(\"threequarterbathnbr\")\n",
    "  typeconstructiontypeid = tf.contrib.layers.real_valued_column(\"typeconstructiontypeid\")\n",
    "  unitcnt = tf.contrib.layers.real_valued_column(\"unitcnt\")\n",
    "  yardbuildingsqft17 = tf.contrib.layers.real_valued_column(\"yardbuildingsqft17\")\n",
    "  garagecarcnt = tf.contrib.layers.real_valued_column(\"garagecarcnt\")\n",
    "\n",
    "\n",
    "#, 'yardbuildingsqft26', 'yearbuilt', 'numberofstories', 'structuretaxvaluedollarcnt', 'taxvaluedollarcnt', 'assessmentyear', 'landtaxvaluedollarcnt',\n",
    "\n",
    "  yardbuildingsqft26 = tf.contrib.layers.real_valued_column(\"yardbuildingsqft26\")\n",
    "  yearbuilt = tf.contrib.layers.real_valued_column(\"yearbuilt\")\n",
    "  numberofstories = tf.contrib.layers.real_valued_column(\"numberofstories\")\n",
    "  structuretaxvaluedollarcnt = tf.contrib.layers.real_valued_column(\"structuretaxvaluedollarcnt\")\n",
    "  taxvaluedollarcnt = tf.contrib.layers.real_valued_column(\"taxvaluedollarcnt\")\n",
    "  assessmentyear = tf.contrib.layers.real_valued_column(\"assessmentyear\")\n",
    "  landtaxvaluedollarcnt = tf.contrib.layers.real_valued_column(\"landtaxvaluedollarcnt\")\n",
    "\n",
    "# 'taxamount', 'taxdelinquencyyear', 'censustractandblock']        \n",
    "  taxamount = tf.contrib.layers.real_valued_column(\"taxamount\")\n",
    "  taxdelinquencyyear = tf.contrib.layers.real_valued_column(\"taxdelinquencyyear\")\n",
    "  censustractandblock = tf.contrib.layers.real_valued_column(\"censustractandblock\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #spsrse \n",
    "# ['hashottuborspa', 'fireplaceflag', 'taxdelinquencyflag']]        \n",
    "  hashottuborspa = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"hashottuborspa\", hash_bucket_size=100)\n",
    "  fireplaceflag = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"fireplaceflag\", hash_bucket_size=100)\n",
    "  taxdelinquencyflag = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"taxdelinquencyflag\", hash_bucket_size=100)\n",
    "\n",
    "\n",
    "\n",
    "  # Wide columns and deep columns.\n",
    "  # continues columns feeding\n",
    "  wide_columns = [airconditioningtypeid, architecturalstyletypeid, basementsqft, bathroomcnt, bedroomcnt, buildingclasstypeid, buildingqualitytypeid, calculatedbathnbr, decktypeid, finishedfloor1squarefeet, calculatedfinishedsquarefeet, finishedsquarefeet12, finishedsquarefeet13, finishedsquarefeet15, finishedsquarefeet50, finishedsquarefeet6, fips, fireplacecnt, fullbathcnt, garagecarcnt, garagetotalsqft, heatingorsystemtypeid, latitude, longitude, lotsizesquarefeet, poolcnt, poolsizesum, pooltypeid10, pooltypeid2, pooltypeid7, propertylandusetypeid, rawcensustractandblock, regionidcity, regionidcounty, regionidneighborhood, regionidzip, roomcnt, storytypeid, threequarterbathnbr, typeconstructiontypeid, unitcnt, yardbuildingsqft17, yardbuildingsqft26, yearbuilt, numberofstories, structuretaxvaluedollarcnt, taxvaluedollarcnt, assessmentyear, landtaxvaluedollarcnt, taxamount, taxdelinquencyyear, censustractandblock]\n",
    "  #categorical columns + continuous feeding\n",
    "  deep_columns = [\n",
    "                    tf.contrib.layers.embedding_column(hashottuborspa, dimension=100),\n",
    "                    tf.contrib.layers.embedding_column(fireplaceflag, dimension=100),\n",
    "                    tf.contrib.layers.embedding_column(taxdelinquencyflag, dimension=100),\n",
    "                    airconditioningtypeid, architecturalstyletypeid, basementsqft, bathroomcnt, bedroomcnt, buildingclasstypeid, buildingqualitytypeid, calculatedbathnbr, decktypeid, finishedfloor1squarefeet, calculatedfinishedsquarefeet, finishedsquarefeet12, finishedsquarefeet13, finishedsquarefeet15, finishedsquarefeet50, finishedsquarefeet6, fips, fireplacecnt, fullbathcnt, garagecarcnt, garagetotalsqft, heatingorsystemtypeid, latitude, longitude, lotsizesquarefeet, poolcnt, poolsizesum, pooltypeid10, pooltypeid2, pooltypeid7, propertylandusetypeid, rawcensustractandblock, regionidcity, regionidcounty, regionidneighborhood, regionidzip, roomcnt, storytypeid, threequarterbathnbr, typeconstructiontypeid, unitcnt, yardbuildingsqft17, yardbuildingsqft26, yearbuilt, numberofstories, structuretaxvaluedollarcnt, taxvaluedollarcnt, assessmentyear, landtaxvaluedollarcnt, taxamount, taxdelinquencyyear, censustractandblock\n",
    "                ]\n",
    "\n",
    "  if model_type == \"wide\":\n",
    "    m = tf.contrib.learn.LinearClassifier(model_dir=model_dir,\n",
    "                                          feature_columns=wide_columns)\n",
    "  elif model_type == \"deep\":\n",
    "    m = tf.contrib.learn.DNNClassifier(model_dir=model_dir,\n",
    "                                       feature_columns=deep_columns,\n",
    "                                       hidden_units=[100, 50])\n",
    "  else:\n",
    "    m = tf.contrib.learn.DNNLinearCombinedClassifier(\n",
    "        model_dir=model_dir,\n",
    "        linear_feature_columns=wide_columns,\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_hidden_units=[100, 50],\n",
    "        fix_global_step_increment_bug=True)\n",
    "  return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_fn(x_train,y_train ):\n",
    "  \"\"\"Input builder function.\"\"\"\n",
    "  # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "  # the values of that column stored in a constant Tensor.\n",
    "  continuous_cols = {k: tf.constant(np.nan_to_num(x_train[k])) for k in CONTINUOUS_COLUMNS}\n",
    "  # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "  # to the values of that column stored in a tf.SparseTensor.\n",
    "  categorical_cols2 = {\n",
    "    k: tf.SparseTensor(\n",
    "        indices=[[i, 0] for i in range(x_train[k].size)],\n",
    "        values=x_train[k].astype(str).fillna('').values,\n",
    "        dense_shape=[x_train[k].size, 1])\n",
    "        for k in CATEGORICAL_COLUMNS}\n",
    "\n",
    "\n",
    "  # Merges the two dictionaries into one.\n",
    "  feature_cols = dict(continuous_cols)\n",
    "  feature_cols.update(categorical_cols)\n",
    "  # Converts the label column into a constant Tensor.\n",
    "  label = tf.constant(y_train)\n",
    "  # Returns the feature columns and the label.\n",
    "  return feature_cols, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_eval(model_dir, model_type, train_steps, train_data, test_data):\n",
    "  \"\"\"Train and evaluate the model.\"\"\"\n",
    "  #train_file_name, test_file_name = maybe_download(train_data, test_data)\n",
    "  train = pd.read_csv('../input/train_2016_v2.csv')\n",
    "  prop = pd.read_csv('../input/properties_2016.csv')\n",
    "  sample = pd.read_csv('../input/sample_submission.csv')\n",
    "\n",
    "\n",
    "  print('Creating training set ...')\n",
    "\n",
    "  df_train = train.merge(prop, how='left', on='parcelid')\n",
    "\n",
    "  x_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode'], axis=1)\n",
    "  y_train = df_train['logerror'].values\n",
    "  print(x_train.shape, y_train.shape)  \n",
    "    \n",
    "\n",
    "  model_dir = tempfile.mkdtemp() if not model_dir else model_dir\n",
    "  print(\"model directory = %s\" % model_dir)\n",
    "\n",
    "  m = build_estimator(model_dir, model_type)\n",
    "  m.fit(input_fn=lambda: input_fn(x_train,y_train), steps=train_steps)\n",
    "  results = m.evaluate(input_fn=lambda: input_fn(x_train,y_train), steps=1)\n",
    "  for key in sorted(results):\n",
    "    print(\"%s: %s\" % (key, results[key]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2827: DtypeWarning: Columns (22,32,34,49,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training set ...\n",
      "(90275, 55) (90275,)\n",
      "model directory = /tmp/tmpe7a0q702\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_session_config': None, '_model_dir': '/tmp/tmpe7a0q702', '_num_worker_replicas': 0, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': None, '_evaluation_master': '', '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_keep_checkpoint_every_n_hours': 10000, '_environment': 'local', '_tf_random_seed': None, '_save_checkpoints_steps': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd45c3de9b0>, '_task_id': 0, '_master': ''}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "SparseTensor(indices=Tensor(\"SparseTensor_8/indices:0\", shape=(90275, 2), dtype=int64), values=Tensor(\"SparseTensor_8/values:0\", shape=(90275,), dtype=string), dense_shape=Tensor(\"SparseTensor_8/dense_shape:0\", shape=(2,), dtype=int64)) must be from the same graph as Tensor(\"Const_17:0\", shape=(90275,), dtype=float64).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-66b988ea4716>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m train_and_eval(FLAGS.model_dir, FLAGS.model_type, FLAGS.train_steps,\n\u001b[0;32m---> 39\u001b[0;31m                  FLAGS.train_data, FLAGS.test_data)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-116-5471eb1e1932>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(model_dir, model_type, train_steps, train_data, test_data)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             instructions)\n\u001b[0;32m--> 289\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    291\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\u001b[0m\n\u001b[1;32m    453\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasic_session_run_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStopAtStepHook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks)\u001b[0m\n\u001b[1;32m    953\u001b[0m       \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m       \u001b[0mmodel_fn_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m       \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOSSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       all_hooks.extend([\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36m_get_train_ops\u001b[0;34m(self, features, labels)\u001b[0m\n\u001b[1;32m   1160\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0mModelFnOps\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m     \"\"\"\n\u001b[0;32m-> 1162\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_eval_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'model_dir'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_fn_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_dir'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelFnOps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.py\u001b[0m in \u001b[0;36m_dnn_linear_combined_model_fn\u001b[0;34m(features, labels, mode, params, config)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mdnn_parent_scope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitervalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         partitioner=dnn_partitioner):\n\u001b[0m\u001b[1;32m    230\u001b[0m       with variable_scope.variable_scope(\n\u001b[1;32m    231\u001b[0m           \u001b[0;34m\"input_from_feature_columns\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mvariable_scope\u001b[0;34m(name_or_scope, default_name, values, initializer, regularizer, caching_device, partitioner, custom_getter, reuse, dtype, use_resource)\u001b[0m\n\u001b[1;32m   1542\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1544\u001b[0;31m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_graph_from_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1545\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1546\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname_or_scope\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[0;34m(op_input_list, graph)\u001b[0m\n\u001b[1;32m   4097\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4098\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4099\u001b[0;31m         \u001b[0m_assert_same_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4100\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[0;34m(original_item, item)\u001b[0m\n\u001b[1;32m   4036\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4037\u001b[0m     raise ValueError(\n\u001b[0;32m-> 4038\u001b[0;31m         \"%s must be from the same graph as %s.\" % (item, original_item))\n\u001b[0m\u001b[1;32m   4039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: SparseTensor(indices=Tensor(\"SparseTensor_8/indices:0\", shape=(90275, 2), dtype=int64), values=Tensor(\"SparseTensor_8/values:0\", shape=(90275,), dtype=string), dense_shape=Tensor(\"SparseTensor_8/dense_shape:0\", shape=(2,), dtype=int64)) must be from the same graph as Tensor(\"Const_17:0\", shape=(90275,), dtype=float64)."
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import tempfile\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n",
    "parser.add_argument(\n",
    "  \"--model_dir\",\n",
    "  type=str,\n",
    "  default=\"\",\n",
    "  help=\"Base directory for output models.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "  \"--model_type\",\n",
    "  type=str,\n",
    "  default=\"wide_n_deep\",\n",
    "  help=\"Valid model types: {'wide', 'deep', 'wide_n_deep'}.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "  \"--train_steps\",\n",
    "  type=int,\n",
    "  default=200,\n",
    "  help=\"Number of training steps.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "  \"--train_data\",\n",
    "  type=str,\n",
    "  default=\"\",\n",
    "  help=\"Path to the training data.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "  \"--test_data\",\n",
    "  type=str,\n",
    "  default=\"\",\n",
    "  help=\"Path to the test data.\"\n",
    ")\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "train_and_eval(FLAGS.model_dir, FLAGS.model_type, FLAGS.train_steps,\n",
    "                 FLAGS.train_data, FLAGS.test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## label, Category, Continuous 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "from six.moves import urllib\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Columns 설정\n",
    "COLUMNS = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n",
    "           \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n",
    "           \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n",
    "           \"income_bracket\"]\n",
    "\n",
    "# Label 설정\n",
    "LABEL_COLUMN = \"label\"\n",
    "\n",
    "#categorical column은 Wide model로 feeding\n",
    "# sparse Tensor 사용\n",
    "CATEGORICAL_COLUMNS = [\"workclass\", \"education\", \"marital_status\", \"occupation\",\n",
    "                       \"relationship\", \"race\", \"gender\", \"native_country\"]\n",
    "\n",
    "#continuous columns은 wide와 deep model로 feeding\n",
    "#realvalue 사용\n",
    "CONTINUOUS_COLUMNS = [\"age\", \"education_num\", \"capital_gain\", \"capital_loss\",\n",
    "                      \"hours_per_week\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census data downlod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 자료를 mlr.cs.umass.edu로 부터 받음\n",
    "def maybe_download(train_data, test_data):\n",
    "  \"\"\"Maybe downloads training data and returns train and test file names.\"\"\"\n",
    "  if train_data:\n",
    "    train_file_name = train_data\n",
    "  else:\n",
    "    train_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "    urllib.request.urlretrieve(\"http://mlr.cs.umass.edu/ml/machine-learning-databases/adult/adult.data\", train_file.name)  # pylint: disable=line-too-long\n",
    "    train_file_name = train_file.name\n",
    "    train_file.close()\n",
    "    print(\"Training data is downloaded to %s\" % train_file_name)\n",
    "\n",
    "  if test_data:\n",
    "    test_file_name = test_data\n",
    "  else:\n",
    "    test_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "    urllib.request.urlretrieve(\"http://mlr.cs.umass.edu/ml/machine-learning-databases/adult/adult.test\", test_file.name)  # pylint: disable=line-too-long\n",
    "    test_file_name = test_file.name\n",
    "    test_file.close()\n",
    "    print(\"Test data is downloaded to %s\" % test_file_name)\n",
    "\n",
    "  return train_file_name, test_file_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide and Deep Model Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_estimator(model_dir, model_type):\n",
    "  \"\"\"Build an estimator.\"\"\"\n",
    "  # Sparse base columns.\n",
    "  gender = tf.contrib.layers.sparse_column_with_keys(column_name=\"gender\",\n",
    "                                                     keys=[\"female\", \"male\"])\n",
    "  education = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"education\", hash_bucket_size=1000)\n",
    "  relationship = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"relationship\", hash_bucket_size=100)\n",
    "  workclass = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"workclass\", hash_bucket_size=100)\n",
    "  occupation = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"occupation\", hash_bucket_size=1000)\n",
    "  native_country = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"native_country\", hash_bucket_size=1000)\n",
    "\n",
    "  # Continuous base columns.\n",
    "  age = tf.contrib.layers.real_valued_column(\"age\")\n",
    "  education_num = tf.contrib.layers.real_valued_column(\"education_num\")\n",
    "  capital_gain = tf.contrib.layers.real_valued_column(\"capital_gain\")\n",
    "  capital_loss = tf.contrib.layers.real_valued_column(\"capital_loss\")\n",
    "  hours_per_week = tf.contrib.layers.real_valued_column(\"hours_per_week\")\n",
    "\n",
    "  # Transformations.\n",
    "  age_buckets = tf.contrib.layers.bucketized_column(age,\n",
    "                                                    boundaries=[\n",
    "                                                        18, 25, 30, 35, 40, 45,\n",
    "                                                        50, 55, 60, 65\n",
    "                                                    ])\n",
    "\n",
    "  # Wide columns and deep columns.\n",
    "  # continues columns feeding\n",
    "  wide_columns = [gender, native_country, education, occupation, workclass,\n",
    "                  relationship, age_buckets,\n",
    "                  tf.contrib.layers.crossed_column([education, occupation],\n",
    "                                                   hash_bucket_size=int(1e4)),\n",
    "                  tf.contrib.layers.crossed_column(\n",
    "                      [age_buckets, education, occupation],\n",
    "                      hash_bucket_size=int(1e6)),\n",
    "                  tf.contrib.layers.crossed_column([native_country, occupation],\n",
    "                                                   hash_bucket_size=int(1e4))]\n",
    "  #categorical columns + continuous feeding\n",
    "  deep_columns = [\n",
    "      tf.contrib.layers.embedding_column(workclass, dimension=8),\n",
    "      tf.contrib.layers.embedding_column(education, dimension=8),\n",
    "      tf.contrib.layers.embedding_column(gender, dimension=8),\n",
    "      tf.contrib.layers.embedding_column(relationship, dimension=8),\n",
    "      tf.contrib.layers.embedding_column(native_country,\n",
    "                                         dimension=8),\n",
    "      tf.contrib.layers.embedding_column(occupation, dimension=8),\n",
    "      age,\n",
    "      education_num,\n",
    "      capital_gain,\n",
    "      capital_loss,\n",
    "      hours_per_week,\n",
    "  ]\n",
    "\n",
    "  if model_type == \"wide\":\n",
    "    m = tf.contrib.learn.LinearClassifier(model_dir=model_dir,\n",
    "                                          feature_columns=wide_columns)\n",
    "  elif model_type == \"deep\":\n",
    "    m = tf.contrib.learn.DNNClassifier(model_dir=model_dir,\n",
    "                                       feature_columns=deep_columns,\n",
    "                                       hidden_units=[100, 50])\n",
    "  else:\n",
    "    m = tf.contrib.learn.DNNLinearCombinedClassifier(\n",
    "        model_dir=model_dir,\n",
    "        linear_feature_columns=wide_columns,\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_hidden_units=[100, 50],\n",
    "        fix_global_step_increment_bug=True)\n",
    "  return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feeding function\n",
    "* SparseTensor : 희소 행렬, 듬성듬성한 데이터 등..\n",
    ">TensorFlow represents a sparse tensor as three separate dense tensors: indices, values, and dense_shape.\n",
    "\n",
    "* SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])\n",
    ">[[1, 0, 0, 0]<br>\n",
    ">[0, 0, 2, 0]<br>\n",
    ">[0, 0, 0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_fn(df):\n",
    "  \"\"\"Input builder function.\"\"\"\n",
    "  # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "  # the values of that column stored in a constant Tensor.\n",
    "  continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\n",
    "  # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "  # to the values of that column stored in a tf.SparseTensor.\n",
    "  categorical_cols = {\n",
    "      k: tf.SparseTensor(\n",
    "          indices=[[i, 0] for i in range(df[k].size)],\n",
    "          values=df[k].values,\n",
    "          dense_shape=[df[k].size, 1])\n",
    "      for k in CATEGORICAL_COLUMNS}\n",
    "  # Merges the two dictionaries into one.\n",
    "  feature_cols = dict(continuous_cols)\n",
    "  feature_cols.update(categorical_cols)\n",
    "  # Converts the label column into a constant Tensor.\n",
    "  label = tf.constant(df[LABEL_COLUMN].values)\n",
    "  # Returns the feature columns and the label.\n",
    "  return feature_cols, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_eval(model_dir, model_type, train_steps, train_data, test_data):\n",
    "  \"\"\"Train and evaluate the model.\"\"\"\n",
    "  train_file_name, test_file_name = maybe_download(train_data, test_data)\n",
    "  df_train = pd.read_csv(\n",
    "      tf.gfile.Open(train_file_name),\n",
    "      names=COLUMNS,\n",
    "      skipinitialspace=True,\n",
    "      engine=\"python\")\n",
    "  df_test = pd.read_csv(\n",
    "      tf.gfile.Open(test_file_name),\n",
    "      names=COLUMNS,\n",
    "      skipinitialspace=True,\n",
    "      skiprows=1,\n",
    "      engine=\"python\")\n",
    "\n",
    "  # remove NaN elements\n",
    "  df_train = df_train.dropna(how='any', axis=0)\n",
    "  df_test = df_test.dropna(how='any', axis=0)\n",
    "\n",
    "  df_train[LABEL_COLUMN] = (\n",
    "      df_train[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "  df_test[LABEL_COLUMN] = (\n",
    "      df_test[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "\n",
    "  model_dir = tempfile.mkdtemp() if not model_dir else model_dir\n",
    "  print(\"model directory = %s\" % model_dir)\n",
    "\n",
    "  m = build_estimator(model_dir, model_type)\n",
    "  m.fit(input_fn=lambda: input_fn(df_train), steps=train_steps)\n",
    "  results = m.evaluate(input_fn=lambda: input_fn(df_test), steps=1)\n",
    "  for key in sorted(results):\n",
    "    print(\"%s: %s\" % (key, results[key]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data is downloaded to /tmp/tmpqu1720dj\n",
      "Test data is downloaded to /tmp/tmp5k184ex1\n",
      "model directory = /tmp/tmp7m2hzjfy\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "WARNING:tensorflow:The default stddev value of initializer will change from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" after 2017/02/25.\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_tf_random_seed': None, '_task_id': 0, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_model_dir': '/tmp/tmp7m2hzjfy', '_keep_checkpoint_max': 5, '_save_checkpoints_secs': 600, '_evaluation_master': '', '_num_worker_replicas': 0, '_environment': 'local', '_save_checkpoints_steps': None, '_session_config': None, '_save_summary_steps': 100, '_master': '', '_task_type': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd184459b70>, '_keep_checkpoint_every_n_hours': 10000, '_num_ps_replicas': 0}\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/feature_column.py:2306: calling sparse_feature_cross (from tensorflow.contrib.layers.python.ops.sparse_feature_cross_op) with hash_key=None is deprecated and will be removed after 2016-11-20.\n",
      "Instructions for updating:\n",
      "The default behavior of sparse_feature_cross is changing, the default\n",
      "value for hash_key will change to SPARSE_FEATURE_CROSS_DEFAULT_HASH_KEY.\n",
      "From that point on sparse_feature_cross will always use FingerprintCat64\n",
      "to concatenate the feature fingerprints. And the underlying\n",
      "_sparse_feature_cross_op.sparse_feature_cross operation will be marked\n",
      "as deprecated.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/feature_column.py:2306: calling sparse_feature_cross (from tensorflow.contrib.layers.python.ops.sparse_feature_cross_op) with hash_key=None is deprecated and will be removed after 2016-11-20.\n",
      "Instructions for updating:\n",
      "The default behavior of sparse_feature_cross is changing, the default\n",
      "value for hash_key will change to SPARSE_FEATURE_CROSS_DEFAULT_HASH_KEY.\n",
      "From that point on sparse_feature_cross will always use FingerprintCat64\n",
      "to concatenate the feature fingerprints. And the underlying\n",
      "_sparse_feature_cross_op.sparse_feature_cross operation will be marked\n",
      "as deprecated.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/feature_column.py:2306: calling sparse_feature_cross (from tensorflow.contrib.layers.python.ops.sparse_feature_cross_op) with hash_key=None is deprecated and will be removed after 2016-11-20.\n",
      "Instructions for updating:\n",
      "The default behavior of sparse_feature_cross is changing, the default\n",
      "value for hash_key will change to SPARSE_FEATURE_CROSS_DEFAULT_HASH_KEY.\n",
      "From that point on sparse_feature_cross will always use FingerprintCat64\n",
      "to concatenate the feature fingerprints. And the underlying\n",
      "_sparse_feature_cross_op.sparse_feature_cross operation will be marked\n",
      "as deprecated.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:625: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:Casting <dtype: 'int64'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'int64'> labels to bool.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmp7m2hzjfy/model.ckpt.\n",
      "INFO:tensorflow:loss = 124.58, step = 1\n",
      "INFO:tensorflow:global_step/sec: 7.3165\n",
      "INFO:tensorflow:loss = 2.71791, step = 101 (13.665 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into /tmp/tmp7m2hzjfy/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.534838.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/feature_column.py:2306: calling sparse_feature_cross (from tensorflow.contrib.layers.python.ops.sparse_feature_cross_op) with hash_key=None is deprecated and will be removed after 2016-11-20.\n",
      "Instructions for updating:\n",
      "The default behavior of sparse_feature_cross is changing, the default\n",
      "value for hash_key will change to SPARSE_FEATURE_CROSS_DEFAULT_HASH_KEY.\n",
      "From that point on sparse_feature_cross will always use FingerprintCat64\n",
      "to concatenate the feature fingerprints. And the underlying\n",
      "_sparse_feature_cross_op.sparse_feature_cross operation will be marked\n",
      "as deprecated.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/feature_column.py:2306: calling sparse_feature_cross (from tensorflow.contrib.layers.python.ops.sparse_feature_cross_op) with hash_key=None is deprecated and will be removed after 2016-11-20.\n",
      "Instructions for updating:\n",
      "The default behavior of sparse_feature_cross is changing, the default\n",
      "value for hash_key will change to SPARSE_FEATURE_CROSS_DEFAULT_HASH_KEY.\n",
      "From that point on sparse_feature_cross will always use FingerprintCat64\n",
      "to concatenate the feature fingerprints. And the underlying\n",
      "_sparse_feature_cross_op.sparse_feature_cross operation will be marked\n",
      "as deprecated.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/feature_column.py:2306: calling sparse_feature_cross (from tensorflow.contrib.layers.python.ops.sparse_feature_cross_op) with hash_key=None is deprecated and will be removed after 2016-11-20.\n",
      "Instructions for updating:\n",
      "The default behavior of sparse_feature_cross is changing, the default\n",
      "value for hash_key will change to SPARSE_FEATURE_CROSS_DEFAULT_HASH_KEY.\n",
      "From that point on sparse_feature_cross will always use FingerprintCat64\n",
      "to concatenate the feature fingerprints. And the underlying\n",
      "_sparse_feature_cross_op.sparse_feature_cross operation will be marked\n",
      "as deprecated.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:625: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:Casting <dtype: 'int64'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'int64'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2017-08-03-10:20:56\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp7m2hzjfy/model.ckpt-200\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-08-03-10:20:57\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 0.841779, accuracy/baseline_label_mean = 0.236226, accuracy/threshold_0.500000_mean = 0.841779, auc = 0.87867, auc_precision_recall = 0.70997, global_step = 200, labels/actual_label_mean = 0.236226, labels/prediction_mean = 0.263047, loss = 0.537796, precision/positive_threshold_0.500000_mean = 0.692775, recall/positive_threshold_0.500000_mean = 0.593344\n",
      "accuracy: 0.841779\n",
      "accuracy/baseline_label_mean: 0.236226\n",
      "accuracy/threshold_0.500000_mean: 0.841779\n",
      "auc: 0.87867\n",
      "auc_precision_recall: 0.70997\n",
      "global_step: 200\n",
      "labels/actual_label_mean: 0.236226\n",
      "labels/prediction_mean: 0.263047\n",
      "loss: 0.537796\n",
      "precision/positive_threshold_0.500000_mean: 0.692775\n",
      "recall/positive_threshold_0.500000_mean: 0.593344\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n",
    "parser.add_argument(\n",
    "  \"--model_dir\",\n",
    "  type=str,\n",
    "  default=\"\",\n",
    "  help=\"Base directory for output models.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "  \"--model_type\",\n",
    "  type=str,\n",
    "  default=\"wide_n_deep\",\n",
    "  help=\"Valid model types: {'wide', 'deep', 'wide_n_deep'}.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "  \"--train_steps\",\n",
    "  type=int,\n",
    "  default=200,\n",
    "  help=\"Number of training steps.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "  \"--train_data\",\n",
    "  type=str,\n",
    "  default=\"\",\n",
    "  help=\"Path to the training data.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "  \"--test_data\",\n",
    "  type=str,\n",
    "  default=\"\",\n",
    "  help=\"Path to the test data.\"\n",
    ")\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "train_and_eval(FLAGS.model_dir, FLAGS.model_type, FLAGS.train_steps,\n",
    "                 FLAGS.train_data, FLAGS.test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
